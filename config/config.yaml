
dataloader:
  data_dir: data
  batch_size: 2 # Because grad accumulation steps > 1, we're simulating a batch size of 64

model:
  vocab_size: 10240 # The tokenizer uses 10000 tokens only, but this is a better number because it's divisble by many powers of 2
  embedding_dim: 768
  n_layers: 12
  heads: 12
  head_size: 64
  block_size: 512

trainer:
  steps: 1466
  gradient_accumulation_steps: 2
  epochs: 2
  val_steps: 4
  init_lr: 1e-3
  grad_clip: 1.0
  checkpoint_dir: checkpoints