
dataloader:
  data_dir: data
  batch_size: 2 # Because grad accumulation steps > 1, we're simulating a batch size of 64
  shuffle: True
  num_workers: 0
  #prefetch_factor: 2 # Should be >0 in case multiprocessing is enabled, i.e. num_workers > 0
  drop_last: False

model:
  vocab_size: 10240 # The tokenizer uses 10000 tokens only, but this is a better number because it's divisble by many powers of 2
  embedding_dim: 768
  n_layers: 12
  heads: 12
  head_size: 64
  block_size: 1024

trainer:
  steps: 734
  gradient_accumulation_steps: 2
  epochs: 1
  val_steps: 4
  init_lr: 1e-3
  checkpoint_dir: checkpoints