
dataloader:
  data_dir: data
  batch_size: 2 # Because grad accumulation steps > 1, we're simulating a batch size of 64

model:
  vocab_size: 10240 # The tokenizer uses 10000 tokens only, but this is a better number because it's divisble by many powers of 2
  embedding_dim: 768
  n_layers: 12
  heads: 12
  head_size: 64
  block_size: 256

trainer:
  steps: 2933
  gradient_accumulation_steps: 2
  epochs: 1
  val_steps: 4
  init_lr: 6e-4
  grad_clip: 1.0
  checkpoint_dir: checkpoints

inference:
  path: 'checkpoints/ckpt.pt'
